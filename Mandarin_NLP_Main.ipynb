{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandarin NLP Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will start by importing neccessary packages and files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import the dataframes files we will use and concat them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = pd.read_csv(\"Data/doc1.csv\")\n",
    "two = pd.read_csv(\"Data/doc2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only interested in the Answer column so let's filter it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [one, two]\n",
    "data = pd.concat(frames) \n",
    "data = data[\"Answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also interested in filtering out certain \"stop words\" in Chinese or words that do not make sense to include in the context of the question. It doesn't make sense to leave feedback that says \"very good\" or \"很好“ so we will leave words and phrases like those out.\n",
    "Since the comments that do not fit the context of the question consist of short a short series of characters, let's limit the list to 4 characters and above and remove any comments with the words \"very good\" and \"don't know\" in them since\n",
    "they do not fit the context of this question. \n",
    "\n",
    "Let's also get rid of the number that we see in the set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Rid of Out-of-Context Comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_list = (filter(lambda x : len(x) > 4 , data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_list = pd.DataFrame(clean_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first strip any extra whitespaces from the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_list[0] = clean_list[0].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "cleaner_list = []\n",
    "\n",
    "for i in clean_list[0]:\n",
    "    if \"好\" in i:\n",
    "        pass\n",
    "    if \"很好\" in i:\n",
    "        pass\n",
    "    elif \"没有\" in i:\n",
    "        pass\n",
    "    elif i.isdigit():\n",
    "        pass\n",
    "    else:\n",
    "        cleaner_list.append(i)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have removed digits and text that was out of context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['知乐很不错',\n",
       " '贴近第一线员工，关心一线员工',\n",
       " '希望员工反应的问题能有所改变',\n",
       " '把问题公开，透明，调查的什么，解决的什么，大家都不知道。',\n",
       " '知乐是工人的心底之声',\n",
       " '深入员工工作现场，实地了解员工想法',\n",
       " '回头看看问题是不是根本解决了。',\n",
       " '让人数在多些，回了解到更多的问题',\n",
       " '积极参加。',\n",
       " '不是我们想想的1',\n",
       " '因为任性，所以有钱???',\n",
       " '想回家，但确实不怎么安全',\n",
       " '科学的探讨与研究，其本身就含有至美，其本身给人的愉快就是报酬；所以我在我的工作里面寻得了快乐',\n",
       " '关于公司企业文化认同',\n",
       " '多关心一下管理模式',\n",
       " '调查了就得，实行才好',\n",
       " '热情这东西其实也很脆弱，耗完了耐心，攒够了失望，剩余的就只有疲惫和冷漠了',\n",
       " '实事求是！坚持原则！',\n",
       " '停车场需改进',\n",
       " '调查很全面，细致',\n",
       " '员工反馈的问题得有个闭环反馈。',\n",
       " '有些题目太笼统',\n",
       " '可不可以和员工有一次面对面的交流，',\n",
       " '感觉不错，就是问题可以再多一些。',\n",
       " '改善速度点',\n",
       " '照片不硬性要求就更好了',\n",
       " '问题太频繁。没时间做',\n",
       " '希望一直都在',\n",
       " '多做点有实际意义的事',\n",
       " '多一些提完建议后改善的示例',\n",
       " '频率一周一次好点',\n",
       " '提高员工参与积极性，奖品发放次数少，奖品反应度不高。可以定制价格在1元左右的笔，可能更能提高积极性。',\n",
       " '关注一下员工心理健康',\n",
       " '无纸张调查',\n",
       " '多看看员工怎么说',\n",
       " '停车场还需要再次改造',\n",
       " '问题更新可以快些',\n",
       " '增加知识面',\n",
       " '多些选择题',\n",
       " '每个人的意见都能被重视',\n",
       " '希望多多发问题吧',\n",
       " '洗澡房和更衣室在一块不好里面朝适',\n",
       " '反馈根本不给解决 ，有意思吗？',\n",
       " '更衣室生锈的更换',\n",
       " '继续为员工提供更好的意见',\n",
       " '现在就可以的',\n",
       " '及时处理，让员工相信我们知乐第一时间解决问题就可以了',\n",
       " '多多贴进现实',\n",
       " '对一个问题有点反复',\n",
       " '设立个小信箱，可以让大家提一些对工作生活有改善的意见或建议！',\n",
       " '餐厅吃饭饭桌希望可以及时清理',\n",
       " '知乐社区不错以后多多支持',\n",
       " '能不能减少加班',\n",
       " '让更多的人参与',\n",
       " '每天干完活下班推迟',\n",
       " '天天发饮料雪糕少发',\n",
       " '反馈的意见公示出来，那些解决，那些未解决。',\n",
       " '关于车间人员和后勤人员老是感觉不一样，车间员工像是低后勤人员一等！有些事情不如后勤人员好办事！',\n",
       " '还可以，谢谢知乐，希望以后多帮员工解决问题！',\n",
       " '深入工人基层了',\n",
       " '定期开员工沟通会，了解员工心想',\n",
       " '化成车间酸气太重希望能改善一下',\n",
       " '食堂饭菜质量和停车场地面整好就不孬了，谢谢',\n",
       " '现场很重要。',\n",
       " '改善后跟踪一下。比如食堂的问题好了两天又和过去没什么区别。回检一下问题是不是真正的解决，或者是暂时的解决。如果说是暂时解决，那做这些调查又有什么意义。',\n",
       " '伙食还没解决',\n",
       " '食堂能够多加点汤类',\n",
       " '可以让更多员工加入，能聆听到更多的员工心声。',\n",
       " '中午餐厅汤能否提前一小时，天热，汤也热',\n",
       " '关于后勤人员打扫厕所的时间需要调整一下不要在员工休息时间打扫厕所',\n",
       " '能不能来接我们下班',\n",
       " '中班能不吃剩菜吗',\n",
       " '停车场能不能有点遮阳的东西',\n",
       " '你一天的爱心可能带来别人一生的感谢。',\n",
       " '餐厅饮食，餐厅工作人员打饭不戴口罩给工人打饭不卫生，有时夜班还有人吸烟一边打饭',\n",
       " '员工反应的问题是不是都解决了，解决的结果可以公告一下吗',\n",
       " '袜子有的放到橱子门上，有的自带吃的，不去食堂吃饭，在便衣室吃饭。',\n",
       " '反应的问题怎么处理的在群里通知一下',\n",
       " '多关心一下管理模式',\n",
       " '别人是高处不胜寒，我们是低处纳百川',\n",
       " '停车场积水过多，建议搭建车棚，或水泥地面',\n",
       " '假期加班如何算薪酬',\n",
       " '公司企业微信中每个员工的信息员工与高管的不对等，员工的都有电话号码信息，高管的一个也查不到，什么意思？怕员工打电话被骚扰？',\n",
       " '希望能继续努力下去',\n",
       " '车间温度太高',\n",
       " '能否多听一些员工的心声，真的能畅所欲言吗？！',\n",
       " '公司能配班车吗？',\n",
       " '反应的问题，处理再迅速点',\n",
       " '问题可以加一点',\n",
       " '工资相差大',\n",
       " '增加工人提问版',\n",
       " '关注员工的需求',\n",
       " '太多了太多了',\n",
       " '尽量少一些视频照片、因工作车间不同手机使用不便',\n",
       " '请关注一下，北门夜间送货大车停放',\n",
       " '要有纸张调查',\n",
       " '女员工是不是能有生理期休息',\n",
       " '劳保手套等质量问题',\n",
       " '贴近生活我非常喜欢',\n",
       " '工作劳保鞋不够穿，每年多配一双',\n",
       " '请关心一下员工的在厂时间，在厂时间需要多长。',\n",
       " '还好，没什么特别的要反馈，谢谢',\n",
       " '有的地方的灯不亮了（如:路灯，厕所灯）',\n",
       " '员工劳动强度',\n",
       " '希望多些更实际的    比如    更衣室衣橱生锈的更换',\n",
       " '工人有力量',\n",
       " '员工的心声',\n",
       " '最好将问题闭环情况给予公示',\n",
       " '多关注员工的工作量，天太热了']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaner_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use LatentDirichletAllocation and TfidfVectorizer to find the most important topics. Let's change n_components to 5 so that we can see the top five topics in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "[('餐厅吃饭饭桌希望可以及时清理', 1.1998173962053942), ('继续为员工提供更好的意见', 1.199817396196032), ('希望一直都在', 1.199817396184001), ('关注员工的需求', 1.199817396182469), ('改善速度点', 1.1998173961804879), ('有些题目太笼统', 1.1998173961789544), ('员工的心声', 1.1998173961777667), ('希望员工反应的问题能有所改变', 1.1998173961767513), ('多多贴进现实', 1.1998173961736007), ('每天干完活下班推迟', 1.1998173961667207)]\n",
      "Topic 1:\n",
      "[('现场很重要', 1.199817728968526), ('员工反馈的问题得有个闭环反馈', 1.1998177289677414), ('你一天的爱心可能带来别人一生的感谢', 1.199817728966825), ('照片不硬性要求就更好了', 1.1998177289662006), ('知乐很不错', 1.199817728963202), ('希望能继续努力下去', 1.199817728962725), ('洗澡房和更衣室在一块不好里面朝适', 1.1998177289582856), ('中班能不吃剩菜吗', 1.1998177289538614), ('伙食还没解决', 1.1998177289507705), ('可不可以和员工有一次面对面的交流', 1.1998177289477587)]\n",
      "Topic 2:\n",
      "[('车间温度太高', 1.199820389952296), ('频率一周一次好点', 1.1998203899506268), ('无纸张调查', 1.1998203899497495), ('多些选择题', 1.199820389945481), ('关注一下员工心理健康', 1.199820389945404), ('停车场还需要再次改造', 1.1998203899401145), ('问题可以加一点', 1.1998203899334707), ('回头看看问题是不是根本解决了', 1.1998203899303492), ('增加工人提问版', 1.1998203899257482), ('食堂能够多加点汤类', 1.1998203899248185)]\n",
      "Topic 3:\n",
      "[('多关心一下管理模式', 2.199836112645355), ('工资相差大', 1.19981125364362), ('天天发饮料雪糕少发', 1.1998112536434722), ('假期加班如何算薪酬', 1.1998112536367376), ('要有纸张调查', 1.199811253630436), ('化成车间酸气太重希望能改善一下', 1.1998112536252643), ('问题更新可以快些', 1.1998112536238612), ('让更多的人参与', 1.1998112536043457), ('多看看员工怎么说', 1.199811253591079), ('更衣室生锈的更换', 1.1998112535864351)]\n",
      "Topic 4:\n",
      "[('积极参加', 1.1998224917211633), ('女员工是不是能有生理期休息', 1.199822491714758), ('贴近生活我非常喜欢', 1.199822491704071), ('增加知识面', 1.199822491701092), ('不是我们想想的1', 1.1998224916986895), ('多一些提完建议后改善的示例', 1.1998224916937394), ('现在就可以的', 1.1998224916929228), ('反应的问题怎么处理的在群里通知一下', 1.199822491692762), ('员工劳动强度', 1.199822491690021), ('知乐是工人的心底之声', 1.1998224916833449)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer().fit(cleaner_list)\n",
    "\n",
    "data_vectorized = vectorizer.transform(cleaner_list)\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=5).fit(data_vectorized)\n",
    "\n",
    "def print_topics(model, vectorizer):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-10 - 1:-1]])\n",
    "        \n",
    "\n",
    "print_topics(lda_model, vectorizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see the 5 most popular topics with comments in their categories according to the LDA model. However, since this is quite a small data set of around 100 observations, \n",
    "it's only a guide that can help us find some patterns, but can't be completely relied on. To better improve this model, we would need a lot more data and to use a tokenizer and cut chinese Chinese text with an package called Jieba. This is especially the case given the structure of the Chinese language. \n",
    "\n",
    "Below we will export the dataframe, then use https://www.wordclouds.com/ to create our word cloud that can be seen accessing the \"chinese_cloud\" png in the Images folder of this repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.DataFrame(cleaner_list)\n",
    "new_data.to_csv(\"/Users/jessicaparker/Desktop/Data/Chinese_NLP_Project/main_chinese_translations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use a Google API to translate all of the text at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "translations = []\n",
    "\n",
    "for column in new_data.columns:\n",
    "    # Unique elements of the column\n",
    "    unique_elements = new_data[column].unique()\n",
    "    \n",
    "    for element in unique_elements:\n",
    "        # Adding all the translations to a dictionary (translations)\n",
    "        translations.append(translator.translate(element).text)\n",
    "        \n",
    "translations_df = pd.DataFrame(translations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing English Text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuation and Tokenizing Text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take the translated comments, remove the punctuation, stopwords,  tokenize, amd lemmatize the text so that we have only words that can show context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all the stopwords that we will remove from the text because they don't add to meaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's remove all of the punctuation, stop words, and tokenize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations_df[0]=translations_df[0].str.replace('[^\\w\\s]','').str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sent = str(translations_df[0])\n",
    "  \n",
    "stop_words = set(stopwords.words('english')) \n",
    "   \n",
    "filtered_sentence = [] \n",
    "  \n",
    "test =  nltk.sent_tokenize(example_sent)\n",
    "\n",
    "for w in test: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "        \n",
    "\n",
    "\n",
    "translations_df['tokenized_sents'] = translations_df.apply(lambda row: nltk.word_tokenize(row[0]), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the text is clean and tokenized. This will make removing stop wrods and lemmatizing, or getting root words, text much easier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words and Lemmatizing Text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we remove the stop words here, we will lemmatize the text. This will give us root words my turning words like 'employees' into 'employee'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations_df['tokenized_sents'] = translations_df['tokenized_sents'].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in text])\n",
    "\n",
    "# df = pd.DataFrame(['this was cheesy', 'she likes these books', 'wow this is great'], columns=['text'])\n",
    "translations_df['text_lemmatized'] = translations_df['tokenized_sents'].apply(lemmatize_text)\n",
    "\n",
    "\n",
    "\n",
    "translations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the difference in all the columns that have been taken in each step. The last column \"text_lemmatized\" features the cleanest text, so we will use this for the wordcloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see if we can extract any topics from the English text. Since words in Chinese have no spcaes between them, the results will be a bit different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer().fit(translations_df['text_lemmatized'])\n",
    "\n",
    "data_vectorized = vectorizer.transform(translations_df['text_lemmatized'])\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=3).fit(data_vectorized)\n",
    "\n",
    "def print_topics(model, vectorizer):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-10 - 1:-1]])\n",
    "        \n",
    "\n",
    "print_topics(lda_model, vectorizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the major topics. However, again this is simply a guide and would need much more data to gain greater accuracy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a work cloud the features the sizes of the words according to their frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Define a function to plot word cloud\n",
    "def plot_cloud(wordcloud):\n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(40, 30))\n",
    "    # Display image\n",
    "    plt.imshow(wordcloud) \n",
    "    # No axis details\n",
    "    plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(width = 3000, height = 2000,  max_words=600, random_state=1, background_color='salmon', colormap='Pastel1', collocations=False).generate(str(translations_df['text_lemmatized']))\n",
    "# Plot\n",
    "                                                                                                                                               \n",
    "plot_cloud(wordcloud)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
